{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Семнар 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "В данном семинаре будем классифицировать твиты на те, которые нравятся/не нравятся людям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Данные\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Download dependings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.14.0-py2.py3-none-any.whl (46kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "\n",
    "pip.main(['install', 'tqdm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "tweets = pandas.DataFrame.from_csv(\"Tweets data.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sentiment                                      SentimentText\n",
      "ItemID                                                              \n",
      "1               0                       is so sad for my APL frie...\n",
      "5002            1       very very very happy and slightly skinny  xx\n",
      "10012           1  &quot;I feel better now, better than James Bro...\n",
      "15012           1  ...what can I say ....what a surprise...http:/...\n",
      "20012           0                                     @ ponyp: hey! \n",
      "25012           1       @404world yes I'm going to find one..pronto \n",
      "30012           0  @AdamJonesey mmm id love some parma violets ri...\n",
      "35012           1  @AlTheYid Roast beef and wasabi... yummmmmmmmm...\n",
      "40012           0  @amandafortier ahhh. yeah. kinda. I tweet from...\n",
      "45012           0                 @antdeshawn ... the pain is worst \n",
      "50012           1  @Ashtarte Just be your usual supportive self. ...\n",
      "55012           0  @azizabatini86 and that's fine--u kno, the dry...\n",
      "60012           0       @Bellemorda I miss you!!! Get back on soon. \n",
      "65012           0  @BrandyWandLover yer so was i hun. sum guy was...\n",
      "70012           0  @Andreicaaat Aww damn I can watch your vid on ...\n",
      "75012           1                      @anorangegal You've got mail \n",
      "80012           1  @cantyka hallo juga  thanks ya udah di follow....\n",
      "85012           0  @cbhamby easy there tiger...you should have co...\n",
      "90012           1  @clickjow depois tu pode ver outras sï¿½ries. ...\n",
      "95012           0  @cianw I've never really been able to eat muss...\n"
     ]
    }
   ],
   "source": [
    "print(tweets[::5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = tweets[\"SentimentText\"]\n",
    "Y = tweets[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts.index = np.arange(texts.shape[0])\n",
    "Y.index = np.arange(Y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Посмотрим на твит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Давайте приведём текст твита в нижний регистр, а также избавимся от всех символов, кроме английских букв:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'           omgaga im sooo  im gunna cry ive been at this dentist since  i was suposed  just get a crown put on mins'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'[^a-zA-Z ]+', '', texts[3].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = list(map(lambda r: re.sub(r'[^a-zA-Z ]+', '', r.lower()), texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = np.array(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['                     is so sad for my apl friend',\n",
       "       '                   i missed the new moon trailer',\n",
       "       '              omg its already  o',\n",
       "       '           omgaga im sooo  im gunna cry ive been at this dentist since  i was suposed  just get a crown put on mins'], \n",
       "      dtype='<U755')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model 0. Признаки - буквы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Возьмем в качестве признаков буквы и опишем каждый твит, как количевство вхождений каждой буквы в него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = []\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyz\"    \n",
    "    for l in letters:\n",
    "        features.append(text.count(l))  \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "X = []\n",
    "for text in texts:\n",
    "    X.append(extract_features(text))\n",
    "X = numpy.array(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Научим на наших признаках простую линейную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#порежем данные\n",
    "X_train = X[:train_size]\n",
    "Y_train = Y[:train_size]\n",
    "X_test = X[train_size:]\n",
    "Y_test = Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение: 0.553202961799\n",
      "Тест: 0.552706238528\n"
     ]
    }
   ],
   "source": [
    "Y_pred_train = lr.predict_proba(X_train)[:, 1]\n",
    "Y_pred_test = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Обучение:\",roc_auc_score(Y_train,(Y_pred_train > 0.5)))\n",
    "print(\"Тест:\", roc_auc_score(Y_test,(Y_pred_test > 0.5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  Opinion mining\n",
    "\n",
    "Давайте использовуем нашу модель чтобы понять, что людям сейчас нравится/не нравится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "with open(\"testdata.txt\") as fin:\n",
    "    test_tweets = fin.read()[:-1].split('\\n')\n",
    "test_tweets = numpy.array(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_ops = []\n",
    "for tweet in test_tweets:\n",
    "    X_ops.append(extract_features(tweet))\n",
    "X_ops = numpy.array(X_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sentiments = <предскажите эмоциональную окраску x_ops>\n",
    "sentiments = lr.predict_proba(X_ops)[:,1]\n",
    "sentiment_order = numpy.argsort(-sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Теперь посмотрим на наиболее любимые/ненавистные людям вещи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.901476836514 : i love sa manthaaaaa aaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaa.. < nononeofyouknowherandit'snottheoneyourthinkingabout >.. \n",
      "\n",
      "0.882174650388 : gnun ata talaga pag nageenejoy ka at ngeenjoy ka sa mga kasama mo. you guys make every difficult things in school easy, kc alam ko nandyan kyo to back everyone up. \n",
      "\n",
      "0.826829838596 : I really like the Davis Square, Porter Square, and Harvard Square areas. \n",
      "\n",
      "0.815749205072 : apparently Harvard College is the undergraduate program of Harvard University, i thought Harvard College was some crappy college playing off of the real thing.. \n",
      "\n",
      "0.805262833876 : * ugly * Mit s5 auf DVD von Angel hab ich ja jetzt endlich alle Buffy UND Angel Staffeln auf DVd, yay!: \n",
      "\n",
      "0.797770466925 : u baggin on my boy jake cuz if u is i could straight fuck you azz up lil bitch holla ata playa.. \n",
      "\n",
      "0.79627790423 : AAA ya se empezare dandoles la grata noticia q se me fue la caspa q tenia detras de la cabeza aquel horrible dia. \n",
      "\n",
      "0.785457638773 : You'll be able to tell really quick that I like the Dallas Mavericks if you don't know that already! \n",
      "\n",
      "0.778283743265 : I love Angelina Jolie, Heath Ledger, Jake Gyllenhall, Arnold Vosloo, Faruza Balk, Jason Wiles, Molly Price, Julia Roberts, Chris Meloni, Mariska Hargitay, Brendan Fraiser, Mel Gibson, Julian McMahon, Dennis Leary, Phillip Seymour Hoffman, Sean Patrick Thomas and more... \n",
      "\n",
      "0.775451267207 : I want to get a volkswagen golf, because you can literally run them off of vegetable oil for free from restaurants. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_ids = sentiment_order[:10]\n",
    "\n",
    "for tweet, score in zip(test_tweets[best_ids], sentiments[best_ids]):\n",
    "    print(score,':',tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.268062821465 : but NO. no one posted'cept cory saying somtine was up my butt how rude i just mit shut this site down so i don't have to deal with it... \n",
      "\n",
      "0.266235597401 : stupid stupid stupid stupid stupid stupid southwest airlines. \n",
      "\n",
      "0.261735544188 : I spent a good bit of time racing around the city on my bike, and last night I went to see Mission Impossible 3 since Tom Cruise is awesome. \n",
      "\n",
      "0.249126714331 : \" You know every time I come around your London, London Bridge, it's goin down like London, London, London, it's goin down I'm comin, comin, comin... \n",
      "\n",
      "0.243382192239 : substantiated debt consolidation loans gm mastercard customizing debt elimination He's simply too inept otherwise.. \n",
      "\n",
      "0.237165226724 : I wanted to test drive both the Stang and the Scion, but when we hit the Toyota lot I spotted the stalker man from my previous visit and busted an immediate bitch. \n",
      "\n",
      "0.231410601486 : the first time, i got to spend time with sansan and tito robbie in the san francisco museum of modern art ( which was awesome, btw. \n",
      "\n",
      "0.231039424927 : i remember i hated shanghai last year i didnt want to move i wanted to die nd i cried everyday nd hated everyone hated SAS and i didnt want to go there or be in shanghai. \n",
      "\n",
      "0.230877034803 : Most of this is related to the fact that what I'm doing here in London is good, but it isn't convincing me it's what I should be doing with my life. \n",
      "\n",
      "0.195484252187 : apart from the shopsssssssssssssss n shops and endless shops on the streets, looking pretty exciting but Shanghai is soooo sucking boring to me now!!!!!!!!!!!.. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "worst_ids = sentiment_order[-10:]\n",
    "\n",
    "for tweet, score in zip(test_tweets[worst_ids], sentiments[worst_ids]):\n",
    "    print(score,':',tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model 1. Bag-of-words & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Аналогично предыдущей модели будем описывать текст, как вектор из количества вхождений каждого признака, только теперь вместо букв, признаками будут являться слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tvorogme/.anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Будем тестировать модель с помощью кросс-валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.836, ACC: 0.763\n",
      "ROC-AUC: 0.838, ACC: 0.764\n",
      "ROC-AUC: 0.840, ACC: 0.767\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(X.shape[0], n_iter=3, test_size=0.3)\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], Y[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(Y[test_ids], preds), \n",
    "                                        accuracy_score(Y[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Важность признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Как уже упоминалось ранее, веса признаков в линейной модели в случае, если признаки отмасштабированы, характеризуют степень их влияния на значение целевой переменной. В задаче классификации текстов, кроме того, признаки являются хорошо интерпретируемыми, поскольку каждый из них соответствует конкретному слову. Изучим влияние конкретных слов на значение целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f_weights = zip(vectorizer.get_feature_names(), lr.coef_[0])\n",
    "f_weights = sorted(f_weights, key=lambda i: i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "musicmonday, 2.560364951179168\n",
      "worries, 2.167659833533782\n",
      "congratulations, 2.0753534207506212\n",
      "chuckle, 2.0032833341327168\n",
      "woohoo, 1.9899849963193705\n",
      "critic, 1.9843612781839177\n",
      "wwwiamsoannoyedcom, 1.9592470065670284\n",
      "followfriday, 1.9269647960484764\n",
      "welcome, 1.925830347196484\n",
      "doll, 1.8618247680085374\n",
      "boyislost, 1.8513132707008306\n",
      "quotit, 1.8497473342272837\n",
      "goodsex, 1.8460538038797283\n",
      "yey, 1.8179764729054693\n",
      "smiling, 1.781149065263116\n",
      "bellacullen, 1.7757361013107607\n",
      "cloudstrife, 1.7658511676341042\n",
      "annjj, 1.733208788645981\n",
      "booshtukka, 1.7319636823489437\n",
      "stoked, 1.7247584291060731\n",
      "satisfied, 1.6806216524716966\n",
      "thank, 1.6802451474639342\n",
      "combustiblesong, 1.6744409245966165\n",
      "arabidopsis, 1.6476211492744646\n",
      "launch, 1.6474884814853406\n",
      "aileenu, 1.6396597498964292\n",
      "cities, 1.6171500868242967\n",
      "bing, 1.6134311955138243\n",
      "heh, 1.6035984178309213\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,30):\n",
    "    print(\"{0}, {1}\".format(f_weights[-i][0], f_weights[-i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurting, -1.9281762034130823\n",
      "fml, -1.9357244124150907\n",
      "unfortunate, -1.9401259010058471\n",
      "boooo, -1.94208181758656\n",
      "broken, -1.9444756326085972\n",
      "hurts, -1.956152551553251\n",
      "pakcricket, -2.0084920441404255\n",
      "sick, -2.0157574339105184\n",
      "heartbreaking, -2.070465744505251\n",
      "pricey, -2.0776284668767446\n",
      "ruined, -2.09427070007355\n",
      "storms, -2.1003701909114287\n",
      "amazingphoebe, -2.105247967257527\n",
      "unfortunately, -2.1133462280121877\n",
      "argh, -2.1204122192984802\n",
      "miss, -2.137825575143884\n",
      "nooo, -2.2365164777141993\n",
      "cancelled, -2.255755521647335\n",
      "frustrating, -2.263412179194482\n",
      "missing, -2.2865498498114634\n",
      "depressing, -2.2891605529180796\n",
      "rip, -2.2982709259545375\n",
      "sadly, -2.4360409622155084\n",
      "gutted, -2.4364451664095244\n",
      "poor, -2.4703206637998294\n",
      "bummer, -2.4854141948688393\n",
      "sucks, -2.565759323143935\n",
      "sad, -3.1297322599724455\n",
      "inaperfectworld, -3.4437137640059095\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(1,30)):\n",
    "    print(\"{0}, {1}\".format(f_weights[i][0], f_weights[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model 2. TFIDF & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ещё один способ работы с текстовыми данными — TF-IDF (Term Frequency–Inverse Document Frequency). Как и в bag-of-words, признаками будут являться слова, но теперь каждый текст будет описываться не векотром с количеством вхождений каждого из слов, а некоторй статистикой tfidf. Рассмотрим коллекцию текстов $D$. Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте:  \n",
    "\n",
    "$$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$\n",
    "\n",
    "где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n",
    "2. Inverse Document Frequency - инверсия частоты, с которой некоторое слово встречается в документах коллекции\n",
    "\n",
    "$$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$ \n",
    "\n",
    "где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$, а $\\left|D\\right|$ - количество докуметов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину: \n",
    "\n",
    "$$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n",
    "\n",
    "Отметим, что значение $\\text{tf}(t, d)$ корректируется для часто встречающихся общеупотребимых слов при помощи значения $\\text{idf}(t, D).$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.851, ACC: 0.773\n",
      "ROC-AUC: 0.848, ACC: 0.769\n",
      "ROC-AUC: 0.849, ACC: 0.771\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(X.shape[0], n_iter=3, test_size=0.3)\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], Y[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(Y[test_ids], preds), \n",
    "                                        accuracy_score(Y[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thanks, 4.98617560872693\n",
      "thank, 4.171159804580989\n",
      "welcome, 4.001053433432517\n",
      "followfriday, 3.9934022824762434\n",
      "great, 3.8782399852062706\n",
      "wait, 3.5130270247128754\n",
      "musicmonday, 3.4573661739856973\n",
      "awesome, 3.3101598804289356\n",
      "worries, 3.2736362581684317\n",
      "amazing, 3.0613245804017315\n",
      "yay, 3.050170538706173\n",
      "good, 3.02087464694146\n",
      "sweet, 2.9639760673966355\n",
      "hehe, 2.9310283810101008\n",
      "happy, 2.9267614358149694\n",
      "cool, 2.877626189806358\n",
      "congratulations, 2.8146480394775564\n",
      "love, 2.74640082883442\n",
      "glad, 2.731931461400761\n",
      "you, 2.730167384923653\n",
      "haha, 2.7188719727051476\n",
      "congrats, 2.7069063292305846\n",
      "nice, 2.695781643228622\n",
      "smile, 2.6646671867799316\n",
      "hi, 2.628709592123032\n",
      "worry, 2.6046885574117415\n",
      "cute, 2.5649139419820677\n",
      "enjoy, 2.5621717801501993\n",
      "hello, 2.510816830623742\n",
      "...\n",
      "damn, -3.1957719301698924\n",
      "raining, -3.196937268578495\n",
      "broke, -3.2263360201451623\n",
      "doesnt, -3.240518825092542\n",
      "upset, -3.2697723364047846\n",
      "horrible, -3.2719409020221\n",
      "bummer, -3.38222258783503\n",
      "no, -3.458224688760571\n",
      "cry, -3.496476229089421\n",
      "sigh, -3.4983611525773424\n",
      "not, -3.507290271710603\n",
      "ugh, -3.5936062914596625\n",
      "hurts, -3.7483793692131866\n",
      "lost, -3.7582288142147986\n",
      "sick, -3.9417032211791567\n",
      "unfortunately, -3.964774929216827\n",
      "hate, -4.199026921689477\n",
      "didnt, -4.2899676177209365\n",
      "dontyouhate, -4.499790438445032\n",
      "cant, -4.575425508749761\n",
      "missing, -4.6088863217372396\n",
      "sadly, -4.694234607604274\n",
      "missed, -4.7198497525697345\n",
      "inaperfectworld, -4.8327277914715365\n",
      "wish, -5.1978773516491295\n",
      "poor, -5.354997223998607\n",
      "sorry, -5.591368082371154\n",
      "miss, -5.807082051592529\n",
      "sucks, -6.096409065363216\n"
     ]
    }
   ],
   "source": [
    "f_weights = zip(vectorizer.get_feature_names(), lr.coef_[0])\n",
    "f_weights = sorted(f_weights, key=lambda i: i[1])\n",
    "for i in range(1,30):\n",
    "    print(\"{0}, {1}\".format(f_weights[-i][0], f_weights[-i][1]))\n",
    "    \n",
    "print('...')\n",
    "for i in reversed(range(1,30)):\n",
    "    print(\"{0}, {1}\".format(f_weights[i][0], f_weights[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Улучшение модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Что можно попробовать? Каждый текст содержит слова, которые никак не влияют на смысл, в английском языке такими словами могут являться предлоги, союзы, местоимения  и тп. Попробуем удалить такие слова и посмотреть на результат классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Задание 1:\n",
    "Удалите из текста stop-words, обучите и протестируйте модель на кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your code there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Удалось улучшить результат?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Что еще? Как в английском, так и в русском языках слова имеют различные формы, чтобы не учитывать каждую форму в модели, хотелось бы однокоренные слова привести к одному слову. Разберем два популярных способа: стемминг и лемматизация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Stemming – это процесс нахождения основы слова. В результате применения данной процедуры однокоренные слова, как правило, преобразуются к одинаковому виду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat smile\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "print(stemmer.stem(\"cats\"), stemmer.stem(\"smiling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                   i missed the new moon trailer'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i miss the new moon trailer\n"
     ]
    }
   ],
   "source": [
    "sen = \" \".join(list(map(lambda x: stemmer.stem(x), texts[1].split())))\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Задание 2:\n",
    "Воспользуйтесь стеммингом для обработки слов в текстах, обучите и протестируйте модель на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Удалось улучшить результат?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Лемматизация — процесс приведения слова к его нормальной форме (лемме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('maxent_treebank_pos_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if is_adjective(tag):\n",
    "        return wn.ADJ\n",
    "    elif is_noun(tag):\n",
    "        return wn.NOUN\n",
    "    elif is_adverb(tag):\n",
    "        return wn.ADV\n",
    "    elif is_verb(tag):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'           omgaga im sooo  im gunna cry ive been at this dentist since  i was suposed  just get a crown put on mins'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omgaga im sooo im gunna cry ive be dentist i be suposed just get crown put min\n"
     ]
    }
   ],
   "source": [
    "pos = nltk.pos_tag(texts[3].split())\n",
    "sen = \" \".join([lmtzr.lemmatize(p[0], penn_to_wn(p[1])) for p in pos if penn_to_wn(p[1]) != None])\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Задание 3: \n",
    "Воспользуйтесь лемматизацией для обработки слов в текстах, обучите и протестируйте модель на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Удалось улучшить результат?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Задание 4:\n",
    "Кратко опишите результаты. Какой метод оказался самым эффективным?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Ответ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
